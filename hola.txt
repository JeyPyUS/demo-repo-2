def main(configuration_file=DEFAULT_CONFIG_FILE, environment=DEFAULT_ENVIRONMENT, log_level=DEFAULT_LOG_LEVEL):
    '''
    Main method which executes the extraction, parsing and loading process for each file, loading the data into STG and finally to DWH.
    '''

    # Project path
    PROJECT_PATH = os.path.abspath(os.path.dirname(__file__))

    # Getting the configuration from the .ini file
    gen_config = cfg.read_section(section = 'globalposition_general', filepath = os.path.join(PROJECT_PATH, configuration_file))
    sftp_aladdin_config = cfg.read_section(section = 'aladdin_sftp', filepath = os.path.join(PROJECT_PATH, configuration_file))
    s3_config = cfg.read_section(section = 'globalposition_s3', filepath = os.path.join(PROJECT_PATH, configuration_file))
    load_config = cfg.read_section(section = 'globalposition_load', filepath = os.path.join(PROJECT_PATH, configuration_file))
    trnf_config = cfg.read_section(section = 'globalposition_trnf', filepath = os.path.join(PROJECT_PATH, configuration_file))

    # Preparing some SFTP variables
    dict_sftp = {'hostname' : sftp_aladdin_config['sftp_hostname'],
             'port' : sftp_aladdin_config['sftp_port'],
             'username' : sftp_aladdin_config['sftp_username'],
             'password' : sftp_aladdin_config['sftp_password'],
    }

    sftp_path_input = sftp_aladdin_config['sftp_path_input']
    sftp_path_processed = sftp_aladdin_config['sftp_path_processed']
    sftp_path_error = sftp_aladdin_config['sftp_path_error']
    sftp_pattern = sftp_aladdin_config['sftp_pattern'].split(',')

    # Preparing some S3 variables
    s3_bucket = s3_config['aws_s3_bucket']
    s3_input_folder = s3_config['aws_s3_prefix_input']
    s3_processed_folder = s3_config['aws_s3_prefix_processed']
    s3_not_processed_folder = s3_config['aws_s3_prefix_not_processed']

    xml_field_name = [x.strip() for x in load_config['xml_field_name'].split(",")]
    bbdd_field_name = [x.strip() for x in load_config['bbdd_field_name'].split(",")]
    dict_xml_bbdd_field = dict(zip(xml_field_name, bbdd_field_name))

    stg_schema = load_config['stg_schema']
    stg_table = load_config['stg_table']
    tmp_schema = trnf_config['tmp_schema']
    tmp_table = trnf_config['tmp_table']
    dwh_schema = trnf_config['dwh_schema']
    dwh_table = trnf_config['dwh_table']

    uniq_cols = [x.strip() for x in trnf_config['uniq_cols'].split(",")]
    scd_cols = [x.strip() for x in trnf_config['scd_cols'].split(",")]
    sum_cols = [x.strip() for x in trnf_config['sum_cols'].split(",")] if trnf_config['sum_cols'] else []
    avg_cols = [x.strip() for x in trnf_config['avg_cols'].split(",")] if trnf_config['avg_cols'] else []
    max_cols = [x.strip() for x in trnf_config['max_cols'].split(",")] if trnf_config['max_cols'] else []
    min_cols = [x.strip() for x in trnf_config['min_cols'].split(",")] if trnf_config['min_cols'] else []

    # Log initialization
    logging.config.fileConfig(os.path.join(COMMON_PATH, 'Config', f'{gen_config["log_config_file"]}'), defaults={
                            'logfilename': os.path.join(COMMON_PATH, 'Logs', f'{basename(os.path.dirname(__file__))}.log').replace('\\','/')})

    logger = logging.getLogger()
    logger.setLevel(log_level)

    logger.info('Script initiated')
    starttime = time()

    # Getting the general configuration in Common for the connection to Infinity database
    conn_redshift_dict = cfg.read_section(section='redshift_infinity_' + environment)

    # Creating the connection to Infinity database
    engine_redshift = db_utils.get_engine_redshift(conn_redshift_dict)

    # Checking the connection and getting metadata information for the tables that we are going to use
    try:
        connection = engine_redshift.connect()
        meta = MetaData(connection)
        meta.reflect(bind=engine_redshift, schema=stg_schema, only=[stg_table])
        meta.reflect(bind=engine_redshift, schema=dwh_schema, only=[dwh_table])
    except Exception as e:
        logger.error(f'Script stopped in {time()-starttime} seconds due to an error on the connection to the database.\nError: {e}')
        return None


    ######################################################
    # TRANSFERING FILES FROM ALADDIN SFTP TO INFINITY S3 #
    ######################################################

    logger.info('Transfering files from Aladdin SFTP and uploading them to S3')

    try:
        s3_utils.upload_from_sftp(dict_sftp, sftp_path_input, sftp_path_processed, sftp_path_error, sftp_pattern, s3_bucket, s3_input_folder)
    except Exception as e:
        logger.error(f'An error has occurred while extracting from Aladdin SFTP to Infinity S3. Error: {e}')
        return None

    logger.info('Transfer completed')


    # Searching and getting files in the folder on S3
    logger.info('Searching new files to extract in S3')

    s3_files = {}

    try:
        s3_files = s3_utils.get_from_s3(s3_bucket, s3_input_folder)

        if not s3_files:
            logger.info('There is no new files to extract')
        else:
            logger.info(f'There is new files and they have been extracted: {[f for f in s3_files]}')
    except Exception as e:
        logger.error(f'An error ocurred while extracting the files. Error: {e}')

    if s3_files:

        for file in s3_files:

            logger.info(f'--- TAR.GZ: {file} ---')

            logger.info('Extracting in-memory the content of the tar.gz file')

            dict_xml_files = {}

            try:
                # Extracting the XML files inside the tar.gz
                tar_file = tarfile.open(fileobj=s3_files[file], mode='r:gz')
                for item in tar_file:
                    if item.name.endswith('.xml'):
                        # Save into the dictionary the XML file
                        dict_xml_files[item.name] = io.BytesIO(tar_file.extractfile(item.name).read())
                tar_file.close()
            except (ReadError, BadGzipFile):
                logger.error(f'The file {file} has the wrong format and cannot be decompressed. The file is going to be moved inside S3 to the error folder')
                move_files_in_s3(file, s3_bucket, s3_bucket, s3_input_folder, s3_not_processed_folder)
                continue

            num_xml_files = len(dict_xml_files)
            aux = 0
            xml_error = False
            logger.info(f'Number of XML extracted in-memory: {num_xml_files}')

            # Truncating the stg table, parsing the xml, converting it to dataframe and loading the data to stg and to dwh
            for xml_file in dict_xml_files:

                aux = aux + 1 # We keep track of the XML files that are going to be processed

                logger.info(f'- XML: {xml_file} -')

                # Since we want to process one XML at a time we will perform the truncate before processing each XML
                logger.info('Truncating STG table...')

                # Truncating stg table
                truncate_stg_table(engine_redshift, stg_schema, stg_table, logger)

                # Parsing and loading into STG
                logger.info('Loading data to STG...')

                try:
                    df = xml_to_df_by_level(xml_file, dict_xml_files[xml_file], dict_xml_bbdd_field=dict_xml_bbdd_field)
                    df_to_staging(engine_redshift, meta, df, xml_file, stg_schema, stg_table)
                except Exception as e:
                    logger.error(f'An exception ocurred: {e}')
                    xml_error = True
                    if num_xml_files == aux: # We are going to move the file only when all the XML files are processed in case we have more than one
                        move_files_in_s3(file, s3_bucket, s3_bucket, s3_input_folder, s3_not_processed_folder)
                    continue # continue with the following iteration (file)

                # Loading to DWH
                logger.info('Loading data from STG to DWH...')

                result = True

                logger.info('Executing SCD checks...')
                
                info_cols = {"sum_cols": sum_cols, "avg_cols": avg_cols, "max_cols": max_cols, 
                             "min_cols": min_cols, "uniq_cols": uniq_cols, "scd_cols": scd_cols}
                
                scd_check_result = scd_check(engine_redshift, meta, stg_schema, dwh_schema, stg_table, dwh_table, info_cols)
                if not scd_check_result:
                    xml_error = True
                    if num_xml_files == aux: # We are going to move the file only when all the XML files are processed in case we have more than one
                        move_files_in_s3(file, s3_bucket, s3_bucket, s3_input_folder, s3_not_processed_folder)
                    continue

                df_insert = scd_check_result[0]
                logger.info(f'Number of completely new rows: {df_insert.shape[0]}')

                if df_insert.shape[0] > 0:
                    logger.debug('Loading to DWH completely new rows...')
                    # Preparing the new rows comming from staging which are completely new
                    df = prepare_df_newrows(df_insert)
                    result = df_to_dwh(engine_redshift, meta, df, dwh_schema, dwh_table)

                # Breaking the routine if something happend
                if not result:
                    xml_error = True
                    if num_xml_files == aux: # We are going to move the file only when all the XML files are processed in case we have more than one
                        move_files_in_s3(file, s3_bucket, s3_bucket, s3_input_folder, s3_not_processed_folder)
                    continue

                df_upsert = scd_check_result[1]
                logger.info(f'Number of rows for SCD checking: {df_upsert.shape[0]}')

                if df_upsert.shape[0] > 0:
                    logger.info('Executing SCD checks...')
                    # Creating the dataframe for the update
                    df = prepare_scd_rows(df_upsert, scd_cols)

                    if df.shape[0] > 0:
                        logger.info('Loading deprecated data in temporal table...')
                        # Inserting in a temporal table the rows which must be updated "deprecated"
                        insert_dataframe(engine_redshift, df, tmp_schema, tmp_table, if_exists='replace')
                        # Deprecating old rows
                        logger.info('Deprecating rows in dwh table...')
                        result = update_scd_rows(engine_redshift, tmp_schema, dwh_schema, tmp_table, dwh_table)

                        # Breaking the routine if something happend
                        if not result:
                            xml_error = True
                            if num_xml_files == aux: # We are going to move the file only when all the XML files are processed in case we have more than one
                                move_files_in_s3(file, s3_bucket, s3_bucket, s3_input_folder, s3_not_processed_folder)
                            continue

                        # After the update, we proceed with the new data for the existing rows as if they were also new
                        db_cols = [c.name for c in meta.tables[stg_schema+'.'+stg_table].columns]

                        # Preparing the new rows comming from staging with scd changes
                        logger.info('Updating data in dwh table...')
                        df = prepare_df_newrows(df[db_cols])
                        result = df_to_dwh(engine_redshift, meta, df, dwh_schema, dwh_table)
                    else:
                        logger.info('We don\'t need to update any information on DWH')

                # Breaking the routine if something happend
                if not result:
                    xml_error = True
                    if num_xml_files == aux: # We are going to move the file only when all the XML files are processed in case we have more than one
                        move_files_in_s3(file, s3_bucket, s3_bucket, s3_input_folder, s3_not_processed_folder)
                    continue

                logger.info(f'File {xml_file} successfuly processed')

                if num_xml_files == aux and not xml_error: # We are going to move the file only when all the XML files were processed successfully in case we had more than one
                    # If all the XML files went fine, we will move the compressed file to processed
                    move_files_in_s3(file, s3_bucket, s3_bucket, s3_input_folder, s3_processed_folder)

                elif num_xml_files == aux and xml_error: # In this case, the last XML file was processed successfully but one or more of the previous ones in the same tar.gz went in error
                    move_files_in_s3(file, s3_bucket, s3_bucket, s3_input_folder, s3_not_processed_folder)


    logger.info(f'Script ended successfully in {time()-starttime} seconds')
